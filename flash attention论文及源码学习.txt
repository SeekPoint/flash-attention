flash attention论文及源码学习
已于 2023-07-01 17:30:55 修改

https://blog.csdn.net/KIDGIN7439/article/details/131293535

yknote===应该是1.0.9或者更早的版本


论文
attention计算公式如下
001.png

传统实现需要将S和P都存到HBM，需要占用O(N^2)内存，计算流程为
002.png

因此前向HBM访存为O(Nd + N^2)，通常N远大于d，GPT2中N=1024，d=64。HBM带宽较小，因此访存会成为瓶颈。
003.png

该论文主要出发点就是考虑到IO的影响，降低内存占用和访问，主要贡献点为：

    重新设计了计算流程，使用softmax tiling的方法执行block粒度的计算

    不需要存储矩阵P，只存储归一化因子，再反向的时候可以快速的recompute

softmax tiling的整体流程如下图，
外层第j次循环拿到K矩阵的第j个block kj，内层第i次循环拿到Q矩阵的第i个block Qi，计算得到S和P，然后再和Vj相乘得到Oi
004.png

然后看下如何计算出softmax。考虑数值稳定性的softmax的传统计算流程如下，需要减去当前行的最大值
005.png

这里的max和sum都需要一行的完整结果。

而flash attention的流程基于递推实现block粒度的计算：
006.png

单看S的一行，假设m(x)m为执行到第i个block即S(i)的最大值，现在执行第i + 1个block S(i + 1)，
....yknote截图  008.png
同理对于sum，最后就可以得到softmax，完整流程如下
007.png

因此内存占用为O(N)，假设share mem大小为M，那么对于HBM的访存为O(N^2 * d^2 * M^-1)

A100 Tensor Core
为了加速深度学习里的fc和卷积，nvidia引入了Tensor Core到gpu里，单个sm如下所示
009.png
图 2-1

A100的一个sm有4个Tensor Core，以FP16/FP32混合精度为例，每个Tensor Core每个周期可以计算256个FP16 FMA，即8x4x8的矩阵运算。
除了通过cublas，cudnn等官方库使用Tensor Core之外，
nv还提供了WMMA和mma PTX两种方式使用Tensor Core，由于flash attention用的是mma PTX，所以后续只介绍下mma PTX。
矩阵的乘累加形为D = A * B + C，其中A和B不支持FP32，输入的FP32会被转为同样位宽的TF32，
C和D支持FP32，详细类型见下表，其中mma.sync就是执行了一次矩阵乘累加
010.png
图 2-2

mma为warp-level的操作，矩阵乘由32线程一起完成，但是存储是和cuda core共享，也就是说A和B需要分布式的存储在32线程的寄存器中，
每个线程存储了原始矩阵的一部分，称为一个fragment，这个分布式存储的过程需要用户显式完成，
然后Tensor Core会访问所有线程寄存器完成矩阵运算，以fp16的16x8x16的A为例，数据在warp中的分布如下所示
011.png
图 2-3

假设A的一个tile已经通过LDG从global mem加载到了shared mem中，为了完成上图的数据排布，我们可以使用LDS指令加载数据，
但是由于数据分布不是连续的，所以要执行4次LDS，为了解决这个问题，nvidia提供了一个指令为ldmatrix，可以一跳指令完成16x16的矩阵加载，
流程如下，每个thread读入128b，然后将128b写入到4个lane对应的寄存器中，
以T0为例，会读入矩阵第一行的前8个FP16，写入到T0，T1，T2，T3对应的寄存器中
012.png
图 2-4

013.png
图 2-5

值得注意的是，假设shared mem中为连续存储，这里将发生bank冲突，gpu的shared mem中有32bank，每个bank 4字节，
由于每个线程读取128b，因此每个线程占4个bank，所以整个读取过程将分为4次，
第一次为T0-T7，第二次为T8-T15，第三次为T16-T23，第四次为T24-T31，如果shared mem中为连续存储，
如下图，数字表示原始16x16矩阵中的行和列，那么在第一次读取中，绿色部分为T0读，蓝色部分为T4读，
将发生冲突，shared mem利用率只有一半。
014.png
图 2-6

为了解决这个问题，cutlass使用了xor swizzle的方法避免bank冲突，如下所示
017.png
图 2-7


# 源码流程
。。。。。。

















